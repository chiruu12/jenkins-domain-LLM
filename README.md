# jenkins-domain-LLM (GSoC 2025)

**Project:** Jenkins Domain specific LLM based on actual Jenkins usage using ci.jenkins.io data

[![Build Status](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/your-repo/jenkins-ai-agent)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

## Project Overview

This repository contains the development work for the Google Summer of Code 2025 project focused on building a LLM-powered assistant to diagnose Jenkins build failures.

The initial proposal centered on fine-tuning a domain-specific LLM. However, the project has since evolved to prioritize a more flexible and robust **universal agentic architecture**. This approach lowers the barrier to entry for users and developers, supports a wider range of models (including any future fine-tuned Jenkins LLM), and provides a more extensible foundation for future development. The core goal remains the same: to create a powerful tool that makes debugging Jenkins failures faster and more intuitive.



## Core Features

*   **Multi-Agent Root Cause Analysis:** Utilizes a chain of specialized AI agents (Router, Specialist, Critic) to perform in-depth analysis of Jenkins build logs.
*   **Interactive Debugging:** Offers a chat-based interface for step-by-step guidance and troubleshooting.
*   **Dual-Layer Conversation Memory:** Remembers context from both the current session (short-term) and all past sessions (long-term) to provide highly relevant and personalized responses.
*   **Tools:** Can interact with the Jenkins workspace, query a knowledge base, and even connect to a live Jenkins instance via the MCP plugin.
*   **Secure by Design:** Features an automatic sanitization pipeline to scrub credentials, API keys, and secrets from any data before it is sent to an LLM.
*   **Multi-Provider Support:** Fully configurable to use a wide range of LLM providers, including Google, OpenAI, Cohere, Mistral, Fireworks, and local Sentence Transformers.

## In Action

The agent provides a clean and intuitive command-line interface for all its operations.

**Command-Line Interface (CLI):**
![CLI Demo](Reports/Final_agent_assets/CLI_jen_agent.png)

**Proposed Future Graphical User Interface (GUI):**
![GUI Mockup](Reports/Final_agent_assets/GUI_jen_agent.png)

## Core Architectural Principles

The entire system is built on four fundamental principles that ensure it is powerful, flexible, and secure.

#### 1. Configuration-Driven
The agent's behavior is not hardcoded. Every critical aspect from the available LLM providers to the specific tools an agent can use is defined in a central `config/config.yaml` file. This allows for easy extension and modification without changing the core code.

#### 2. Modular & Extensible
The project features a strict separation of concerns, making it highly maintainable and easy to expand:
- **Models (`/models`):** Each LLM provider is a self-contained class with a consistent interface, loaded dynamically by a factory.
- **Tools (`/tools`):** The agent's capabilities are encapsulated in modular tools.
- **Agents (`agents.py`):** A central `AgentFactory` assembles agents on demand, combining models and tools based on the configuration.
- **Pipelines (`/pipelines`):** High-level, multi-step workflows are defined independently, allowing for complex behaviors like critique-and-refinement loops.

#### 3. Secure by Design
The agent operates with a security-first mindset. All user-provided data, such as build logs and workspace files, is passed through a robust sanitization pipeline.

- **Sanitization:** The `ContentSanitizer` automatically scrubs credentials, API keys, and other secrets.
- **Mapping & Rehydration:** Secrets are replaced with safe placeholders (e.g., `[AWS_KEY_1]`). These are only converted back to their original values in the final report shown to the user, ensuring the LLM never has access to sensitive information.

![Data Sanitization Flow](Reports/Final_agent_assets/data_sanitization_flow.png)

#### 4. Intelligent Memory
The agent features a dedicated, two-layered memory system to maintain context across conversations.
- **Short-Term Memory:** Remembers the last few turns of the current session for immediate context.
- **Long-Term Memory:** All conversations are vectorized and stored in a dedicated FAISS index and SQLite database. This allows the agent to retrieve semantically relevant memories from any past session to inform its current reasoning.

## Architecture Diagrams

#### Component Interaction

This diagram illustrates the key classes and modules and how they are wired together within the application.

![Component Interaction Diagram](Reports/Final_agent_assets/component_interaction_diagram.png)

#### Data Flow Sequence

This sequence diagram shows the step-by-step flow of data for a single user query in an interactive session, from input to memory retrieval, LLM generation, and final storage.

![Data Flow Sequence Diagram](Reports/Final_agent_assets/dataflow_sequence_diagram.png)

## Project Structure Deep Dive

The project is organized to promote modularity and a clear separation of concerns.
```markdown
ğŸ“ jenkins-domain-LLM/
â”œâ”€â”€ ğŸ“„ .env.example
â”œâ”€â”€ ğŸ“„ .gitignore
â”œâ”€â”€ ğŸ“„ README.md  # The main project README file.
â”œâ”€â”€ ğŸ“„ requirements.txt  # A list of all the python dependencies for the project.
â”œâ”€â”€ ğŸ“ Jen_agent  # The main application source code.
â”‚   â”œâ”€â”€ ğŸ“„ .env
â”‚   â”œâ”€â”€ ğŸ“„ agents.py  # Core agent logic and the AgentFactory.
â”‚   â”œâ”€â”€ ğŸ“„ cli.py  # Main application entrypoint using Typer.
â”‚   â”œâ”€â”€ ğŸ“„ data_models.py  # Pydantic models for all structured data (reports, settings, etc.).
â”‚   â”œâ”€â”€ ğŸ“„ engine.py
â”‚   â”œâ”€â”€ ğŸ“„ gui.py
â”‚   â”œâ”€â”€ ğŸ“„ log_manager.py
â”‚   â”œâ”€â”€ ğŸ“„ memory.py  # Implements the dual-layer conversation memory system (SQLite + FAISS).
â”‚   â”œâ”€â”€ ğŸ“„ pipeline.py  # Factory for creating the correct pipeline based on the selected mode.
â”‚   â”œâ”€â”€ ğŸ“„ prompt_examples.py
â”‚   â”œâ”€â”€ ğŸ“„ sanitizer.py
â”‚   â”œâ”€â”€ ğŸ“„ settings.py  # Pydantic models for loading and validating config.yaml.
â”‚   â”œâ”€â”€ ğŸ“„ tests.py
â”‚   â”œâ”€â”€ ğŸ“ Benchmark  # Scripts for running and analyzing agent performance benchmarks.
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ BENCHMARK_REPORT.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ analyze_benchmark.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ curate_benchmark_files.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ generate_questions.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ run_benchmark.py
â”‚   â”‚   â””â”€â”€ ğŸ“ benchmark_data
â”‚   â”‚       â””â”€â”€ ... # Contains raw data, questions, and results for performance benchmarking.
â”‚   â”œâ”€â”€ ğŸ“ RAG_scripts  # Utility scripts, e.g., for ingesting documents into the knowledge base.
â”‚   â”‚   â””â”€â”€ ğŸ“„ ingest_docs.py
â”‚   â”œâ”€â”€ ğŸ“ commands  # Logic for interactive slash commands (e.g., /help, /view).
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ base.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ handlers.py
â”‚   â”œâ”€â”€ ğŸ“ config  # Centralized configuration for the entire application.
â”‚   â”‚   â””â”€â”€ ğŸ“„ config.yaml
â”‚   â”œâ”€â”€ ğŸ“ docs
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ Jenkins-agent.md
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ agent_interactions.png
â”‚   â”‚   â””â”€â”€ ğŸ“„ agent_interactions.puml
â”‚   â”œâ”€â”€ ğŸ“ models  # LLM provider integrations and the factory for creating them.
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ base.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ utils.py
â”‚   â”‚   â””â”€â”€ ... (and all provider client folders)
â”‚   â”œâ”€â”€ ğŸ“ pipelines  # Defines high-level, multi-step agentic workflows (Operating Modes).
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ base.py
â”‚   â”‚   â””â”€â”€ ... (and all pipeline files)
â”‚   â”œâ”€â”€ ğŸ“ prompts  # All system and tool prompts, organized by agent/pipeline.
â”‚   â”‚   â””â”€â”€ ... (and all prompt folders)
â”‚   â”œâ”€â”€ ğŸ“ test
â”‚   â”‚   â””â”€â”€ ... # Contains sample log files and test artifacts for local development.
â”‚   â”œâ”€â”€ ğŸ“ tools  # Agent capabilities (e.g., file access, knowledge base query).
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ... (and all tool files)
â”‚   â””â”€â”€ ğŸ“ ui  # (Future) Code for the graphical user interface.
â”‚       â””â”€â”€ ğŸ“„ tkinter_display.py
â”œâ”€â”€ ğŸ“ Prototype
â”‚   â””â”€â”€ ... # Contains early-stage prototype code and test files.
â”œâ”€â”€ ğŸ“ Prototype_CLI
â”‚   â””â”€â”€ ... # Contains an early command-line interface prototype.
â””â”€â”€ ğŸ“ Reports
    â””â”€â”€ ... # Contains all generated diagrams, assets, and architectural documents.
```
## Getting Started

Follow these steps to set up and run the Jenkins AI Agent on your local machine.

#### 1. Prerequisites
- Python 3.10 or higher
- Git

#### 2. Clone the Repository
```bash
git clone https://github.com/chiruu12/jenkins-domain-LLM
cd jenkins-domain-LLM/jen_agent
```

#### 3. Set Up Virtual Environment
```bash
python -m venv venv

# Activate it
# On macOS/Linux:
source venv/bin/activate
# On Windows:
.\venv\Scripts\activate
```

#### 4. Install Dependencies
```bash
pip install -r requirements.txt
```

#### 5. Configure the Agent
The agent's configuration is managed through `config/config.yaml` and environment variables.

1.  **Review `config.yaml`**: The default settings are pre-configured to use the local `sentence-transformer` provider for memory. You can customize providers, models, and other settings here.
2.  **Set API Keys**: Create a `.env` file in the project root to store your API keys. Copy the format from the example below:

    ```.env
    # .env.example
    GOOGLE_API_KEY="your-google-api-key"
    OPENAI_API_KEY="your-openai-api-key"
    COHERE_API_KEY="your-cohere-api-key"
    MISTRAL_API_KEY="your-mistral-api-key"
    FIREWORKS_API_KEY="your-fireworks-api-key"
    ```

#### 6. Initialize the vector store
Before running the application for the first time, you need to initialize the vector store for the Agentic RAG.

Run this script in the root of the project to create and populate the vector store:
```bash
./scripts/vector_store.sh
```

#### 7. Run the Application

- ##### CLI 
```bash
python cli.py
```

- ##### UI
```bash
python gui.py
```

## Usage

When you launch the application, you will be guided through a series of prompts to:
1.  Choose whether to use default settings or customize the session (e.g., select a different LLM provider).
2.  Select an operating mode (e.g., `Standard Diagnosis`, `Interactive Debugging`).

Once in an interactive mode, you can type `/help` at any time to see a list of available slash commands for managing your session.

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
